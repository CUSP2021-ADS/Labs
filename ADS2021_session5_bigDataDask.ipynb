{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to big data tools\n",
    "\n",
    "Python packages like numpy, pandas, sklearn, seaborn etc. make the data manipulation and ML tasks very convenient. For most data analysis tasks, the python pandas package is good enough. You can do all sorts of data manipulation and it is compatible with building ML models.\n",
    "\n",
    "But, as your data gets bigger, bigger than what you can fit in the RAM, pandas won’t be sufficient. When it comes to working with really large datasets, the run time can become very high due to memory constraints. The standard libraries like pandas and numpy usually work well if the dataset is small enough (upto 2-3 GBs). Unfortunately, these popular libraries were not designed to scale beyond a single machine and given a large dataset to analyze (like 8/16/32  GB or beyond), it would be difficult to process and model it using standard means. \n",
    "\n",
    "## Dask\n",
    "\n",
    "Dask is popularly known as a ‘parallel computing’ python library that has been designed to run across multiple systems. Dask can efficiently perform parallel computations on a single machine using multi-core CPUs. For example, if you have a quad core processor, Dask can effectively use all 4 cores of your system simultaneously for processing. In order to use lesser memory during computations, Dask keeps the complete data on the disk, and uses chunks of data (smaller parts, rather than the whole data) from the disk for processing. During the processing, the intermediate values generated (if any) are discarded as soon as possible, to save the memory consumption.\n",
    "\n",
    "This way Dask supports the Pandas dataframe and Numpy array data structures to analyze large datasets. Basically, Dask lets you scale pandas and numpy with minimum changes in your code format.\n",
    "\n",
    "Installation: https://docs.dask.org/en/latest/install.html\n",
    "\n",
    "Some additional resources for diving deeper into Dask and common operations:\n",
    "\n",
    "#### Dask documentation: https://docs.dask.org/en/latest/\n",
    "\n",
    "#### Detailed book/tutorial: https://livebook.manning.com/book/data-science-at-scale-with-python-and-dask/about-this-book/\n",
    "\n",
    "#### Parallel computing: https://ckyrkou.medium.com/an-introduction-to-parallel-computing-dffa6b79e57c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask arrays\n",
    "\n",
    "A large numpy array is divided into smaller arrays which, when grouped together, form the Dask array. In simple words, Dask arrays are distributed numpy arrays. Every operation on a Dask array triggers operations on the smaller numpy arrays, each using a core on the machine. Thus all available cores are used simultaneously enabling computations on arrays which are larger than the memory size.\n",
    "\n",
    "<img src=\"array.png\" width=\"500\">\n",
    "\n",
    "\n",
    "A number of numpy arrays are arranged into grids to form a Dask array. While creating a Dask array, you can specify the chunk size which defines the size of the numpy arrays. For instance, if you have 10 values in an array and you give the chunk size as 5, it will return 2 numpy arrays with 5 values each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 762.94 MiB </td>\n",
       "                        <td> 7.63 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (10000, 10000) </td>\n",
       "                        <td> (1000, 1000) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 100 Tasks </td>\n",
       "                        <td> 100 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"120\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"120\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"120\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"120\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"120\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"120\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"84\" x2=\"120\" y2=\"84\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"120\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"108\" x2=\"120\" y2=\"108\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"120\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"120\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"120\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"120\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"120\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"120\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"120\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"120\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"120\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10000</text>\n",
       "  <text x=\"140.000000\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,60.000000)\">10000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000, 10000), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "# example\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a 10000x10000 array of random numbers uniformly distrubuted between 0 and 1. The full object would contain 100M numbers - not impossible, but challenging to handle as a single variable.\n",
    "\n",
    "Dask would represent it as many numpy arrays of size specified by chunk=1000x1000 (or smaller if the array cannot be divided evenly). \n",
    "\n",
    "In this case there are 100 (10x10) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'compute()' function changes the result to a numpy array. If you don't call compute, the output remains as a dask object storing the sequency of suggested computations, but not running those just yet (\"lazy\" approach).\n",
    "\n",
    "#### Important Note: \n",
    "\n",
    "Before calling the compute() function, any dask object remains what is called a Dask 'collection', which essentially stores all the computations called before but not actually performs them. With the compute method, all computations called before are performed simultaneously and the object transform from a dask collection to a concrete value in local memory.\n",
    "\n",
    "It is thus important to know when and when not to call the compute method. The approach often breaks down if we try to bring the entire dataset as output back to local RAM. So if are dealing with data that exceeds machine RAM, it is better not to obtain the entire data as output. However, we can still call the compute method for obtaining low-memory outputs like computations for selected arrays (or columns in a dataframe) or aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29653447, 0.43131085, 0.37298345, ..., 0.90632792, 0.37869041,\n",
       "        0.50675453],\n",
       "       [0.81302411, 0.17387528, 0.848152  , ..., 0.53977867, 0.76026503,\n",
       "        0.75954449],\n",
       "       [0.53122646, 0.56568247, 0.28361891, ..., 0.33282537, 0.63752668,\n",
       "        0.67216828],\n",
       "       ...,\n",
       "       [0.61311915, 0.19781427, 0.98775041, ..., 0.78681004, 0.94992171,\n",
       "        0.7650614 ],\n",
       "       [0.83581085, 0.59416967, 0.84150185, ..., 0.57660925, 0.99142219,\n",
       "        0.77554088],\n",
       "       [0.2167564 , 0.67440457, 0.51694412, ..., 0.81522757, 0.43800305,\n",
       "        0.97738486]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist data in memory\n",
    "If you know you have the available RAM for your dataset then you can persist data in memory. Essentially this operation temporarily saves the data in your computer memory if the size of the data does not exceed your RAM.\n",
    "\n",
    "This allows future computations to be much faster. \n",
    "\n",
    "Let's check how much time does it take to calculate sum of the above array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 40.4 ms, total: 1.06 s\n",
      "Wall time: 185 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49998710.80312676"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate sum and call compute to print the output\n",
    "%time x.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 203 ms, sys: 7.66 ms, total: 210 ms\n",
      "Wall time: 47.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49998710.80312676"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's try the persist() function and then check the time\n",
    "y = x.persist()\n",
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation time almost reduces to 1/10th of the original time after calling the persist() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask dataframes\n",
    "\n",
    "We saw that multiple numpy arrays are grouped together to form a Dask array. Similar to a Dask array, a Dask dataframe consists of multiple smaller pandas dataframes which are stored in disk/cluster as a single dask object. These Pandas DataFrames may live on disk for computing on a single machine, or on many different machines in a cluster. One Dask DataFrame operation triggers many operations on the constituent Pandas DataFrames.\n",
    "\n",
    "A large pandas dataframe splits row-wise to form multiple smaller dataframes. These smaller dataframes are present on a disk of a single machine, or multiple machines (thus allowing to store datasets of size larger than the memory). Each computation on a Dask dataframe parallelizes operations over the different chunks on the dataframe.\n",
    "\n",
    "\n",
    "<img src=\"dataframe.png\" width=\"300\">\n",
    "\n",
    "\n",
    "### Common uses:\n",
    "\n",
    "Dask DataFrame is used in situations where Pandas is commonly needed, usually when Pandas fails due to data size or computation speed.\n",
    "\n",
    "1. Manipulating large datasets, even when those datasets don’t fit in memory\n",
    "2. Accelerating long computations by using many cores\n",
    "3. Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations\n",
    "\n",
    "The APIs offered by the Dask dataframe are very similar in syntax to that of the pandas dataframe.\n",
    "\n",
    "#### Now, let’s perform some basic operations on Dask dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download 2020 January TLC yellow taxi data. Generally, TLC data contains millions of records even for a single month and pandas usually has a hard time reading and performing computations on it. Anyway, pandas either altogether fails to load data for multiple months or takes a huge time reading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  566M  100  566M    0     0   536k      0  0:17:59  0:17:59 --:--:--  810k0  0:37:06  0:00:03  0:37:03  260k 447k      0  0:21:34  0:00:43  0:20:51  663k04  0:02:27  0:19:37  477k1:27  0:02:57  0:18:30  534k     0   450k      0  0:21:26  0:03:11  0:18:15  356k.0M    0     0   445k      0  0:21:41  0:03:29  0:18:12  492k49  0:03:40  0:18:09  352k66M   18  107M    0     0   441k      0  0:21:54  0:04:09  0:17:45  336k  19  109M    0     0   442k      0  0:21:49  0:04:14  0:17:35  512kM    0     0   449k      0  0:21:29  0:04:30  0:16:59  514k1  121M    0     0   451k      0  0:21:23  0:04:34  0:16:49  591k  25  142M    0     0   463k      0  0:20:50  0:05:15  0:15:35  695kM    0     0   471k      0  0:20:28  0:05:39  0:14:49  611k 0  0:20:31  0:06:24  0:14:07  345k 0  0:20:34  0:07:06  0:13:28  516k20:34  0:07:37  0:12:57  537k   37  212M    0     0   469k      0  0:20:35  0:07:43  0:12:52  415k37  0:07:44  0:12:53  361k 0  0:20:34  0:08:02  0:12:32  528k0  0:20:30  0:08:08  0:12:22  599k   0   466k      0  0:20:42  0:08:53  0:11:49  563k0   468k      0  0:20:36  0:09:04  0:11:32  569kM    0     0   470k      0  0:20:31  0:09:41  0:10:50  661k 0     0   480k      0  0:20:07  0:10:19  0:09:48  658k    0     0   481k      0  0:20:04  0:10:52  0:09:12  376k 0     0   480k      0  0:20:06  0:10:56  0:09:10  381k0     0   479k      0  0:20:09  0:11:18  0:08:51  453k479k      0  0:20:09  0:11:19  0:08:50  473k  0     0   476k      0  0:20:17  0:11:56  0:08:21  352k  0     0   483k      0  0:19:59  0:12:51  0:07:08  659k  0:19:32  0:13:40  0:05:52  806kM    0     0   498k      0  0:19:22  0:14:19  0:05:03  627k    0   501k      0  0:19:15  0:14:42  0:04:33  705k0  0:19:01  0:15:40  0:03:21  806k  0:16:03  0:02:53  502k31  0:16:51  0:01:40  806k\n"
     ]
    }
   ],
   "source": [
    "!curl https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv > yellow_tripdata_2020-01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first compare the time taken to read a single month of data by dask and pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.17 ms, sys: 5.93 ms, total: 15.1 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "# read file: 'read_csv()' works just like pandas\n",
    "%time df = dd.read_csv('yellow_tripdata_2020-01.csv', dtype={'trip_distance': float, 'total_amount': float, 'RatecodeID': float, 'VendorID': float, 'passenger_count': float, 'payment_type':float, 'PULocationID':int, 'DOLocationID':int, 'tolls_amount': 'float64'})\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 4.68 s, total: 15.4 s\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-01-01 00:28:15</td>\n",
       "      <td>2020-01-01 00:33:03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.27</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-01-01 00:35:39</td>\n",
       "      <td>2020-01-01 00:43:04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-01-01 00:47:41</td>\n",
       "      <td>2020-01-01 00:53:52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-01-01 00:55:23</td>\n",
       "      <td>2020-01-01 01:00:14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-01 00:01:58</td>\n",
       "      <td>2020-01-01 00:04:16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0       1.0  2020-01-01 00:28:15   2020-01-01 00:33:03              1.0   \n",
       "1       1.0  2020-01-01 00:35:39   2020-01-01 00:43:04              1.0   \n",
       "2       1.0  2020-01-01 00:47:41   2020-01-01 00:53:52              1.0   \n",
       "3       1.0  2020-01-01 00:55:23   2020-01-01 01:00:14              1.0   \n",
       "4       2.0  2020-01-01 00:01:58   2020-01-01 00:04:16              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            1.2         1.0                  N           238           239   \n",
       "1            1.2         1.0                  N           239           238   \n",
       "2            0.6         1.0                  N           238           238   \n",
       "3            0.8         1.0                  N           238           151   \n",
       "4            0.0         1.0                  N           193           193   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0           1.0          6.0    3.0      0.5        1.47           0.0   \n",
       "1           1.0          7.0    3.0      0.5        1.50           0.0   \n",
       "2           1.0          6.0    3.0      0.5        1.00           0.0   \n",
       "3           1.0          5.5    0.5      0.5        1.36           0.0   \n",
       "4           2.0          3.5    0.5      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  \n",
       "0                    0.3         11.27                   2.5  \n",
       "1                    0.3         12.30                   2.5  \n",
       "2                    0.3         10.80                   2.5  \n",
       "3                    0.3          8.16                   0.0  \n",
       "4                    0.3          4.80                   0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check time with pandas \n",
    "%time df1 = pd.read_csv('yellow_tripdata_2020-01.csv', dtype={'trip_distance': float, 'total_amount': float, 'RatecodeID': float, 'VendorID': float, 'passenger_count': float, 'payment_type':float, 'PULocationID':int, 'DOLocationID':int})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the huge difference between dask and pandas just for reading a single month of data (around 6M rows). Dask is ~150 times faster than pandas here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice:\n",
    "The .head() function automatically converts the dask dataframe to pandas dataframe. Unlike Pandas, Dask DataFrames are lazy and so no data is printed. Let's try printing a dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=18</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 18 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type fare_amount    extra  mta_tax tip_amount tolls_amount improvement_surcharge total_amount congestion_surcharge\n",
       "npartitions=18                                                                                                                                                                                                                                                                     \n",
       "                float64               object                object         float64       float64    float64             object        int64        int64      float64     float64  float64  float64    float64        int64               float64      float64              float64\n",
       "                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...\n",
       "...                 ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...\n",
       "                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...\n",
       "                    ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...\n",
       "Dask Name: read-csv, 18 tasks"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that in a dask dataframe, we don't see the inherent values of data but instead only get information about columns and their dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common pandas operations that can be used in dask\n",
    "\n",
    "As mentioned before, The operations offered by the Dask dataframe are very similar to that of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                 float64\n",
       "tpep_pickup_datetime      object\n",
       "tpep_dropoff_datetime     object\n",
       "passenger_count          float64\n",
       "trip_distance            float64\n",
       "RatecodeID               float64\n",
       "store_and_fwd_flag        object\n",
       "PULocationID               int64\n",
       "DOLocationID               int64\n",
       "payment_type             float64\n",
       "fare_amount              float64\n",
       "extra                    float64\n",
       "mta_tax                  float64\n",
       "tip_amount               float64\n",
       "tolls_amount               int64\n",
       "improvement_surcharge    float64\n",
       "total_amount             float64\n",
       "congestion_surcharge     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dtypes operation for checking column dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna and droplicates work the same way here\n",
    "# one difference: some arguments like 'inplace' do not work\n",
    "\n",
    "# lets drop rows where PULocation and DOLocation are not present\n",
    "df = df.dropna(subset=['PULocationID', 'DOLocationID'])\n",
    "\n",
    "# drop duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try making a new column:travel time\n",
    "# first need to make pickup and dropoff time columns as datetime - like pandas, 'to_datetime()' operation works here\n",
    "\n",
    "#convert columns to datetime dtype\n",
    "df['tpep_pickup_datetime'] = dd.to_datetime(df['tpep_pickup_datetime'])\n",
    "df['tpep_dropoff_datetime'] = dd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "# make a travel time column (minutes)\n",
    "df['travel_time'] = df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n",
    "df['travel_time'] = (df['travel_time'].dt.seconds)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: describe-numeric, 98 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              trip_distance total_amount\n",
       "npartitions=1                           \n",
       "                    float64      float64\n",
       "                        ...          ...\n",
       "Dask Name: describe-numeric, 98 tasks"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's try calculating some descriptive statistics for trip distance and total amount\n",
    "\n",
    "df[['trip_distance', 'total_amount']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from before that dask dataframe themselves don't print out enough info. So in order to print required descriptive stats like pandas, we need to convert the above dataframe to pandas dataframe. \n",
    "\n",
    "This can be done by compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.392059e+06</td>\n",
       "      <td>6.392059e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.921246e+00</td>\n",
       "      <td>1.862563e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.324248e+01</td>\n",
       "      <td>1.473465e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.062000e+01</td>\n",
       "      <td>-1.242300e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.600000e-01</td>\n",
       "      <td>1.116000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.600000e+00</td>\n",
       "      <td>1.430000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.910000e+00</td>\n",
       "      <td>1.980000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.102401e+05</td>\n",
       "      <td>4.268300e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_distance  total_amount\n",
       "count   6.392059e+06  6.392059e+06\n",
       "mean    2.921246e+00  1.862563e+01\n",
       "std     8.324248e+01  1.473465e+01\n",
       "min    -3.062000e+01 -1.242300e+03\n",
       "25%     9.600000e-01  1.116000e+01\n",
       "50%     1.600000e+00  1.430000e+01\n",
       "75%     2.910000e+00  1.980000e+01\n",
       "max     2.102401e+05  4.268300e+03"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this could take some time\n",
    "df[['trip_distance', 'total_amount']].describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.5 s, sys: 12.5 s, total: 55.1 s\n",
      "Wall time: 39.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOLocationID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>34512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>85767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>124589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>43923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>3145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DOLocationID\n",
       "PULocationID              \n",
       "1                      754\n",
       "2                        3\n",
       "3                      176\n",
       "4                    10089\n",
       "5                       39\n",
       "...                    ...\n",
       "261                  34512\n",
       "262                  85767\n",
       "263                 124589\n",
       "264                  43923\n",
       "265                   3145\n",
       "\n",
       "[261 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupby operation - works the same as pandas\n",
    "\n",
    "%time df.groupby('PULocationID').count()[['DOLocationID']].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.7 s, sys: 15.1 s, total: 59.7 s\n",
      "Wall time: 48.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>travel_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.901910</td>\n",
       "      <td>2.923386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.200000</td>\n",
       "      <td>17.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.680909</td>\n",
       "      <td>28.174242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.643181</td>\n",
       "      <td>16.061256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.239231</td>\n",
       "      <td>46.370940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>4.155047</td>\n",
       "      <td>21.106464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2.287864</td>\n",
       "      <td>12.977914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2.060650</td>\n",
       "      <td>12.376074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2.674895</td>\n",
       "      <td>14.309626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2.965901</td>\n",
       "      <td>10.957705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              trip_distance  travel_time\n",
       "PULocationID                            \n",
       "1                  0.901910     2.923386\n",
       "2                  7.200000    17.044444\n",
       "3                  7.680909    28.174242\n",
       "4                  2.643181    16.061256\n",
       "5                 18.239231    46.370940\n",
       "...                     ...          ...\n",
       "261                4.155047    21.106464\n",
       "262                2.287864    12.977914\n",
       "263                2.060650    12.376074\n",
       "264                2.674895    14.309626\n",
       "265                2.965901    10.957705\n",
       "\n",
       "[261 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean trip distance and travel time per pickup location\n",
    "\n",
    "%time df[['PULocationID', 'trip_distance', 'travel_time']].groupby('PULocationID').mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that converting to calculating stats and converting to pandas take quite some time. This is because once we call compute(), all operations done above are executed all at once.\n",
    "\n",
    "One way to reduce this time is by assigning some RAM of your machine to the data. This can be done by persist() operation described above.\n",
    "\n",
    "#### Important note:\n",
    "\n",
    "Persist method turns lazy Dask collections into Dask collections with the same metadata, but now with their results fully computed or actively computing in the background. For example a dataframe built up from many lazy calls will now be a new dataframe of the same shape, dtype, chunks, etc., but now with all of those previously lazy tasks either computed in memory as many small dataframes (in the single-machine case) or asynchronously running in the background on a cluster (in the distributed case).\n",
    "\n",
    "Generally, it is better to run persist method regularly among computations if we are dealing with large number of computations in our work (provided we have sufficient memory on your machine). A large number of computations executed at once may overload the memory and task could fail in that case. \n",
    "\n",
    "Now let's try reducing the data size by keeping only the columns we need for further analysis. We can then run persist method to allocate some RAM to the resulting data and perform all computations till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
       "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
       "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
       "       'total_amount', 'congestion_surcharge', 'travel_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select few columns that we need for further\n",
    "df = df[['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', \n",
    "        'trip_distance', 'PULocationID', 'DOLocationID', 'total_amount', 'travel_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist operation to assign some RAM to data\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 'df' still remains a dask object after calling persist method, but all operations done before this point would be computed after calling persist. One further advantage of persist is the time in further computations would be significantly reduced going furhter on. Essentially, persist stores the updated data as a dask object after doing all computations on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try some operations. Notice the computation time would be significantly reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 397 ms, sys: 125 ms, total: 522 ms\n",
      "Wall time: 459 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>travel_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.901910</td>\n",
       "      <td>2.923386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.200000</td>\n",
       "      <td>17.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.680909</td>\n",
       "      <td>28.174242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.643181</td>\n",
       "      <td>16.061256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.239231</td>\n",
       "      <td>46.370940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>4.155047</td>\n",
       "      <td>21.106464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2.287864</td>\n",
       "      <td>12.977914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2.060650</td>\n",
       "      <td>12.376074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2.674895</td>\n",
       "      <td>14.309626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2.965901</td>\n",
       "      <td>10.957705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              trip_distance  travel_time\n",
       "PULocationID                            \n",
       "1                  0.901910     2.923386\n",
       "2                  7.200000    17.044444\n",
       "3                  7.680909    28.174242\n",
       "4                  2.643181    16.061256\n",
       "5                 18.239231    46.370940\n",
       "...                     ...          ...\n",
       "261                4.155047    21.106464\n",
       "262                2.287864    12.977914\n",
       "263                2.060650    12.376074\n",
       "264                2.674895    14.309626\n",
       "265                2.965901    10.957705\n",
       "\n",
       "[261 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean trip distance and travel time per pickup location\n",
    "\n",
    "%time df[['PULocationID', 'trip_distance', 'travel_time']].groupby('PULocationID').mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>borough</th>\n",
       "      <th>shape_area</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>0.0007823067885</td>\n",
       "      <td>Newark Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>0.00486634037837</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>0.000314414156821</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>0.000111871946192</td>\n",
       "      <td>Alphabet City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>0.000497957489363</td>\n",
       "      <td>Arden Heights</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id        borough         shape_area                     zone\n",
       "0            1            EWR    0.0007823067885           Newark Airport\n",
       "1            2         Queens   0.00486634037837              Jamaica Bay\n",
       "2            3          Bronx  0.000314414156821  Allerton/Pelham Gardens\n",
       "3            4      Manhattan  0.000111871946192            Alphabet City\n",
       "4            5  Staten Island  0.000497957489363            Arden Heights"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try the merge operation\n",
    "\n",
    "# read taxi zone file\n",
    "zips = gpd.read_file('taxizone.geojson')\n",
    "zips = zips[['location_id', 'borough', 'shape_area', 'zone']]\n",
    "zips['location_id'] = pd.to_numeric(zips['location_id'])\n",
    "zips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge df with zips file\n",
    "\n",
    "df = df.merge(zips, how='left', left_on='PULocationID', right_on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'number of trips')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEDCAYAAAA/eB+kAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWwklEQVR4nO3deZhldX3n8feHTVAQNbSOEaGVII6ioHQ0KkbE6IMLLgnqOEZFxZ5MxgW3GXnMTGQmmYhGxyjqhFFEDWCi4oYGQ3wgKKvd0KzixjJizNAQF8CEze/8cX5F3y6qq05X1emqPrxfz3OfOts953vuPfWp3/2dc0+lqpAkjc82S12AJGkYBrwkjZQBL0kjZcBL0kgZ8JI0Uga8JI3Usgv4JMcnuT7JZT2Xf0mSK5JcnuSkoeuTpK1Fltt18El+G7gZ+FRV7TvHsnsDfwMcXFU/TfLAqrp+S9QpScvdsmvBV9VZwD9PTkuyV5LTkqxN8s0kj2yzXgd8uKp+2p5ruEtSs+wCfhOOA95QVQcAbwM+0qY/AnhEkrOTnJfkkCWrUJKWme2WuoC5JNkZeDLw2SRTk+/Vfm4H7A0cBOwOnJXkMVX1sy1cpiQtO8s+4Ok+ZfysqvafYd51wPlVdTtwdZLv0QX+t7dgfZK0LC37Lpqq+gVdeL8YIJ392uwv0rXeSbIbXZfNVUtQpiQtO8su4JOcDJwL7JPkuiSvBV4OvDbJxcDlwAva4l8HbkxyBXAG8PaqunEp6pak5WbZXSYpSVocy64FL0laHMvqJOtuu+1WK1euXOoyJGmrsXbt2huqasVM85ZVwK9cuZI1a9YsdRmStNVIcu2m5g3aRZPkfkk+l+TKJN9J8qQhtydJ2mDoFvxfAKdV1WFJdgDuPfD2JEnNYAGfZFfgt4HDAarqNuC2obYnSdrYkF00DwPWA59IclGSjyW5z/SFkqxOsibJmvXr1w9YjiTdswwZ8NsBjwc+WlWPA24B3jF9oao6rqpWVdWqFStmPBEsSZqHIQP+OuC6qjq/jX+OLvAlSVvAYAFfVf8E/CjJPm3SM4ArhtqeJGljQ19F8wbgxHYFzVXAqwfeniSpGTTgq2odsGrIbUiSZrasvskqbU1WvuOrS11CL9e8+7lLXYKWiDcbk6SRMuAlaaQMeEkaKQNekkbKgJekkTLgJWmkDHhJGikDXpJGyoCXpJEy4CVppAx4SRopA16SRsqAl6SRMuAlaaQMeEkaKQNekkbKgJekkTLgJWmkDHhJGikDXpJGyoCXpJEy4CVppAx4SRopA16SRsqAl6SR2m7IlSe5BrgJuBO4o6pWDbk9SdIGgwZ88/SqumELbEeSNMEuGkkaqaEDvoC/S7I2yeqBtyVJmjB0F82BVfXjJA8ETk9yZVWdNblAC/7VAHvsscfA5UjSPcegLfiq+nH7eT3wBeAJMyxzXFWtqqpVK1asGLIcSbpHGSzgk9wnyS5Tw8CzgMuG2p4kaWNDdtE8CPhCkqntnFRVpw24PUnShMECvqquAvYbav2SpNl5maQkjZQBL0kjZcBL0kgZ8JI0Uga8JI2UAS9JI2XAS9JIGfCSNFIGvCSNlAEvSSNlwEvSSBnwkjRSBrwkjZQBL0kjZcBL0kgZ8JI0Uga8JI2UAS9JI2XAS9JIGfCSNFIGvCSNlAEvSSNlwEvSSBnwkjRSBrwkjZQBL0kjNWfAJ3lKkvu04d9P8v4kew5fmiRpIfq04D8K/DLJfsBbgR8Cn+q7gSTbJrkoyanzrFGSNA99Av6OqirgBcCxVfVhYJfN2MabgO/MpzhJ0vz1CfibkhwFvAL4apJtgO37rDzJ7sBzgY/Nv0RJ0nz0CfiXArcCr6mqfwJ2B97bc/0fAP4z8KtNLZBkdZI1SdasX7++52olSXOZM+BbqJ8E3D/JocBtVTVnH3yS5wHXV9XaOdZ/XFWtqqpVK1as6Fu3JGkOfa6iOQK4APhd4DDgvCSv6bHupwDPT3IN8Bng4CR/tYBaJUmbYbsey7wdeFxV3QiQ5NeAc4DjZ3tSVR0FHNWecxDwtqr6/YUUK0nqr08f/I3ATRPjN7VpkqRlrE8L/gfA+Um+BExdLnlJkrcAVNX751pBVZ0JnDn/MiVJm6tPwP+wPaZ8qf3cnGvhJUlb2JwBX1VHb4lCJEmLa5MBn+QDVXVkkq/Qdc1spKqeP2hlkqQFma0F/+n288+3RCGSpMW1yYCvqrVJtgVWV9XLt2BNkqRFMOtlklV1J7Bnkh22UD2SpEXS5yqaq4Czk3wZuGVqYp/LIyVJS2dzLpPchg2XRt7tpKskaXnpE/BXVNVnJyckefFA9UiSFkmfWxUc1XOaJGkZme06+GcDzwEekuSDE7PuC9wxdGGSpIWZrYvmH4E1wPOByXu63wS8eciiJEkLN9t18BcDFyc5qapu34I1SZIWQZ//6GS4S9JWqM9JVknSVmiTAZ/k0+3nm7ZcOZKkxTJbC/6AJL8OvCbJ/ZM8YPKxpQqUJM3PbFfR/G/gG8DD6a6iycS8atMlScvUJlvwVfXBqvq3wPFV9fCqetjEw3CXpGWuz390+o9J9gOe2iadVVWXDFuWJGmh5ryKJskbgROBB7bHiUneMHRhkqSF6XOzsSOAJ1bVLQBJjgHOBT40ZGGSpIXpcx18gDsnxu9k4xOukqRlqE8L/hPA+Um+0MZfCHx8sIokSYuiz0nW9yc5EziwTXp1VV00aFWSpAXr04Knqi4ELhy4FknSIhrsXjRJdkxyQZKLk1ye5OihtiVJurteLfh5uhU4uKpuTrI98K0kf1tV5w24TUlSM2sLPsm2Sc6Yz4qrc3Mb3b49/GfdkrSFzBrwVXUn8Ksku85n5e0PxDrgeuD0qjp/hmVWJ1mTZM369evnsxlJ0gz6dNHcDFya5HTglqmJVfXGuZ7Y/kDsn+R+wBeS7FtVl01b5jjgOIBVq1bZwpekRdIn4E9pj3mrqp+1rp5DgMvmWl6StHB9roP/ZJKdgD2q6rt9V5xkBXB7C/edgGcCx8y/VEnS5uhzs7FDgXXAaW18/yRf7rHuBwNnJLkE+DZdH/ypC6hVkrQZ+nTRvAt4AnAmQFWtSzLn/eDbLYUft5DiJEnz1+eLTrdX1c+nTfvVEMVIkhZPnxb85Un+PbBtkr2BNwLnDFuWJGmh+rTg3wA8mu6bqScDvwCOHLAmSdIi6HMVzS+Bd7Z/9FFVddPwZUmSFqrPVTS/meRS4BK6LzxdnOSA4UuTJC1Enz74jwN/WFXfBEhyIN0/AXnskIVJkhamTx/8nVPhDlBV3wLuGK4kSdJi2GQLPsnj2+A/JPlLuhOsBbyUdk28JGn5mq2L5n3Txv94YtibgknSMrfJgK+qp2/JQiRJi2vOk6ztVr+vBFZOLt/ndsGSpKXT5yqarwHnAZfiLQokaavRJ+B3rKq3DF6JJGlR9blM8tNJXpfkwUkeMPUYvDJJ0oL0acHfBrwXeCcbrp4pYM5bBkuSlk6fgH8r8BtVdcPQxUiSFk+fLpofAL8cuhBJ0uLq04K/BVjX/mn2rVMTvUxSkpa3PgH/xfaQJG1F+twP/pNbohBJ0uLq803Wq5nh3jNV5VU0krSM9emiWTUxvCPwYsDr4CVpmZvzKpqqunHi8eOq+gDw3OFLkyQtRJ8umsdPjG5D16Lv0/KXJC2hPkE9eV/4O4BrgJcMUo0kadH0uYrG+8JL0laoTxfNvYDf4+73g//vw5UlSVqoPrcq+BLwArrumVsmHrNK8tAkZyS5IsnlSd60sFIlSZujTx/87lV1yDzWfQfw1qq6MMkuwNokp1fVFfNYlyRpM/VpwZ+T5DGbu+Kq+klVXdiGbwK+Azxkc9cjSZqfPi34A4HD2zdabwUCVFU9tu9GkqwEHgecP8O81cBqgD322KPvKiVJc+gT8M9eyAaS7Ax8Hjiyqn4xfX5VHQccB7Bq1aq73RJBkjQ/fS6TvHa+K0+yPV24n1hVp8x3PZKkzdenD35ekgT4OPCdqnr/UNuRJM1ssIAHngK8Ajg4ybr2eM6A25MkTRjsnjJV9S26E7KSpCUwZAtekrSEDHhJGikDXpJGyoCXpJEy4CVppAx4SRopA16SRsqAl6SRMuAlaaQMeEkaKQNekkbKgJekkTLgJWmkDHhJGikDXpJGyoCXpJEy4CVppAx4SRopA16SRsqAl6SRMuAlaaQMeEkaKQNekkbKgJekkTLgJWmkDHhJGqnBAj7J8UmuT3LZUNuQJG3akC34E4BDBly/JGkWgwV8VZ0F/PNQ65ckzW7J++CTrE6yJsma9evXL3U5kjQaSx7wVXVcVa2qqlUrVqxY6nIkaTSWPOAlScMw4CVppIa8TPJk4FxgnyTXJXntUNuSJN3ddkOtuKpeNtS6JUlzs4tGkkbKgJekkTLgJWmkDHhJGikDXpJGyoCXpJEy4CVppAx4SRopA16SRsqAl6SRMuAlaaQMeEkaKQNekkbKgJekkTLgJWmkBrsfvKStz8p3fHWpS+jlmnc/d6lL2CrYgpekkTLgJWmkDHhJGikDXpJGyoCXpJEy4CVppAx4SRopA16SRsqAl6SRMuAlaaQMeEkaqUHvRZPkEOAvgG2Bj1XVu4fc3ph4TxBJCzVYCz7JtsCHgWcDjwJeluRRQ21PkrSxIVvwTwB+UFVXAST5DPAC4IohNmaLV5I2lqoaZsXJYcAhVXVEG38F8MSqev205VYDq9voPsB3BylofnYDbljqIhbR2PYHxrdPY9sfGN8+Lbf92bOqVsw0Y8nvB19VxwHHLXUdM0mypqpWLXUdi2Vs+wPj26ex7Q+Mb5+2pv0Z8iqaHwMPnRjfvU2TJG0BQwb8t4G9kzwsyQ7AvwO+POD2JEkTBuuiqao7krwe+DrdZZLHV9XlQ21vIMuy62gBxrY/ML59Gtv+wPj2aavZn8FOskqSlpbfZJWkkTLgJWmk7hEBn+TOJOuSXJzkwiRPXuqa5jJUzUkOT3LsDNPfleRti7GNhZrY96nHO5K8IMkXJ5Y5KskPJsYPTfLlNnxNkkuTXJLkH5LsuQg1VZK/mhjfLsn6JKcuYJ03b+byB00eB0leOOS3w5PsnuRLSb6f5Kokxya511Db24y63pnk8vb+rkvyxDb9yCT37vH8Xsv1rOWg2Y6BJCuTXLYltjWTe0TAA/9SVftX1X7AUcCfTV8gyZJ/J2CarbHmxTK171OPdwPnAL81scyTgF8keWAbf3JbZsrTq+qxwJnAHy1CTbcA+ybZqY0/ky1/2e9BdPs55YV0twFZdEkCnAJ8sar2BvYGdgLeM8T2NqOuJwHPAx7f3t/fAX7UZh8J9Anuvstt9e4pAT/pvsBP4a6/iN9sLb8rkuyY5BOt9XdRkqe35Q5PckqS01pr5j1t+p5tfLck27R1PWuZ1Dzj9ElJnpvk3CS7TUzbK8mFE+N7T423lvHR7RPFpUkeOcC+zqiq1tMF+m+0SQ8BPs+GwHsycPYMTz23LbsYvgZM3WviZcDJUzOSPKG9lhclOSfJPm36jMfOxPP+tH1KOy/Jg9q0Q5Oc39b190kelGQl8AfAm1ur9WnA84H3tvG9krwuybfb+j4/1UpNckKSD7a6rkr3LfO5HAz8a1V9AqCq7gTeDLwyyesz8SkwyalJDmrDz2qvw4VJPptk5zb9gHSfptYm+XqSB7fpZyY5JskFSb6X5Klt+qPbtHWtpb5329yDgRuq6tZW1w1V9Y9J3gj8OnBGkjPaOj6aZE261v7RbdpMy22q5s063pM8LRs+dV6UZJdp81e2390LM/GpvP1On5nkc0muTHJikrR5h7RpFwK/2+N921hVjf4B3AmsA64Efg4c0KYfRNcye1gbfyvd5ZwAjwT+L7AjcDhwFbBrG78WeGhb7gjgs8Dbgb9cRjXPti/HAi8Cvgncvy3zLuBtbfgMYP82/D+BN7ThayaG/5DuDqFDvl9Tj5e26Z8AXkl3S4vPAM+ga1FuB/wM2HGizt3a8AeA1YtQ083AY4HPtddxXXsvTm3z7wts14Z/B/h8G57t2Cng0Db8HuCP2vD92XCF2xHA+6a/R238BOCwifFfmxj+k4n36gS6Y3Qbuhb/D3rs7xuB/zXD9IvoWsDHTkw7tb0WuwFnAfdp0/8L8N+A7ek+Xa1o01/KhmPzzIn9ew7w9234Q8DL2/AOwE5teOf22n8P+AjwtIk67nrf2/gD2s9t23YeO8PxMWPNfY/3acfAV4CnTNS5HbASuKxNuzcbjtG9gTUT6/g53ZdBt6FrlBxId7z8qC0b4G+mttX3MdaP+NP9S1XtD3d9xPtUkn3bvAuq6uo2fCDdgUVVXZnkWuARbd43qurnbR1XAHsCP6qqjyV5MV3rav9lVPNs+3IwsAp4VlX9YoZtfwx4dZK30P0yPmFi3int51rm06Lo5659n+Ycupb6tnS/BBfQBcjjgCur6l8nlj0jyQPogvm/LkZRVXVJa0m/jK41P2lX4JOtpVl0oTZlxmMHuI0uHKF7PZ/ZhncH/rq1cncArqaffZP8CXA/uoD5+sS8L1bVr+g+9T2o5/o212/R/QE5uzVAd6B7n/YB9gVOb9O3BX4y8bzJY2plGz4XeGeS3YFTqur7AFV1c5IDgKcCT6d7nd5RVSfMUM9L0t3raju6lv+jgEt61jxTbXMd72cD709yYqv5urbOKdsDxybZn64R84iJeRdU1XUASda11+Fm4OqpfU93Dmg1m+Ee10VTVefS/dWeujnPLT2feuvE8J20L4m1j8G7t+k7L0aN0y2g5k35IbALGx9gkz5Pd5vn5wFrq+rGiXlTr8Ndr8EWdDZdwD8ZOLeqbqJr5RzExv3v0P3y70nX2jt6EWv4MvDnTHTPNP8DOKOq9gUObXVNmfHYAW6v1oSbNv1DdC3kxwD/Ydq6ZnMC8Pr2vKNnqWGj1NmEK4ADJickuS/wb4Ab2Tg7prYT4PTacO7kUVX12jb98onpj6mqya7Mux1TVXUSXRfUvwBfS3Lw1MJVdWdVnVlVfwy8Hvi96cUneRjwNuAZ1fXVf5WZX8dN1bzJ2jalunNFR9Cdqzh7hi6dNwP/D9iProG1wwzb6bWtvu5xAd9e9G3pDtLpvgm8vC33CGAP5r675THAiXQtyf+zeJVuMM+aZ9uXa+l+KT6V5NHTV9hawl8HPkrXLbJcfIeu//RAuq4C6AL8D5ih/72q7qDrTnhla80vhuOBo6vq0mnTd2XDSdfDF7iNyXW9amL6TXR/mDc1vgvwkyTb0977BfgGcO8kr4S7/r/D++i6964G9k933umhbPiEdx7wlLTzJEnu04697wIr2idRkmw/03E3KcnDgauq6oPAl+i6x0iyz0R/PHSfmq9tw5Ovx33pGkI/b59Ynj3xnMnlNlXzZkuyV1VdWlXH0N2qZXrA7wr8pH2SegXd7/RsrgRWJtmrjb9sc2u6pwT8TlMnP4C/Bl5V3Umj6T4CbJPk0rbc4dVO5swk3Ymu3wSOqaoTgduSvHqZ1DzrvlTVlXQh8NmJA2jSicCvgL9bpP3ZHHfte3u8G6C1ds8Hbqyq29uy5wIP5+4teNpzfkLX2v5Pi1FYVV3XQme69wB/luQiFt76ehfd+7KWjW9L+xXgRe01eSrdeYi3txN6e9F1RZ1P98fuyoUU0F7rFwGHJfk+XePiV1X1p239V9O18j8IXNies57uj9vJSS6he28eWVW3AYcBxyS5mO6P8lyX/b4EuKwd//sCn2rTd6brCruibeNRdK8XdLcQOC3JGVV1MV0j4ErgJDZuAEwuN2PN/V+pjRyZ5LK2ntuBv502/yPAq9pr8Ejm+CTeGlqrga+2k6zXb25B3qpAM0p3TfyuVbUo/dfaurUrPk4GXlRVF861vJYHA153k+QLwF7AwVW1nP6xgaTNYMBL0kjdU/rgJekex4CXpJEy4CVppAx4SRopA16SRur/A6lQybf7u36fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we may also use matplotlib functions with dask \n",
    "\n",
    "tripByBoro = df.groupby('borough').count()[['DOLocationID']].compute()\n",
    "plt.bar(tripByBoro.index, tripByBoro.DOLocationID.values)\n",
    "plt.ylabel('number of trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW task 1\n",
    "\n",
    "Compute and Plot \n",
    "\n",
    "1. Plot average number of trips as bar plots by day of week\n",
    "2. Plot average total fare amount by hour of pick up time\n",
    "3. Average speed by pick up hour (average speed should be calculated as total distance traveled by hour/total travel time by hour. Plot as barplot.\n",
    "4. Report top 5 and bottom 5 pickup locations in terms of a) total distance, b) average speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Scikit-Learn\n",
    "\n",
    "Alternatively, we can use scikit-learn for training models on medium sized data where pandas would usually fail or would take a huge amount of time.\n",
    "\n",
    "Scikit-learn uses joblib for single-machine parallelism. This lets you train most estimators (anything that accepts an n_jobs parameter) using all the cores of your laptop or workstation. Alternatively, Scikit-Learn can use Dask for parallelism. This lets you train those estimators using all the cores of your cluster without significantly changing your code.\n",
    "\n",
    "This is most useful for training large models on medium-sized datasets. You may have a large model when searching over many hyper-parameters, or when using an ensemble method with many individual estimators. For too small datasets, training times will typically be small enough that cluster-wide parallelism isn’t helpful. For too large datasets (larger than a single machine’s memory), the scikit-learn estimators may not be able to cope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use skearn to build a simple linear model with Lasso regularization with trip cost amount as a target variable and using just trip distance as a regressor. We'll use GridSearchCV for tuning the hyperparameter (alpha) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some filtering to remove outliers and wrong values\n",
    "df = df[(df.trip_distance > 0) & (df.trip_distance < 50)]\n",
    "df = df[(df.total_amount > 0) & (df.total_amount < 300)]\n",
    "df = df[(df.travel_time > 0) & (df.travel_time < 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X as features, y as target (note: we need to call compute method, as sklearn only takes numpy arrays)\n",
    "X = df[['trip_distance','travel_time']].values.compute()\n",
    "y = df.total_amount.values.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Scikit-Learn Pipeline\n",
    "\n",
    "Pipeline consist of all the models (or intermediate steps) that we wish to train/perform on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=Pipeline(steps=[('lasso', Lasso())]), n_jobs=-1,\n",
       "             param_grid={'lasso__alpha': array([9.765625e-04, 1.953125e-03, 3.906250e-03, 7.812500e-03,\n",
       "       1.562500e-02, 3.125000e-02, 6.250000e-02, 1.250000e-01,\n",
       "       2.500000e-01, 5.000000e-01, 1.000000e+00, 2.000000e+00,\n",
       "       4.000000e+00, 8.000000e+00, 1.600000e+01, 3.200000e+01,\n",
       "       6.400000e+01, 1.280000e+02, 2.560000e+02, 5.120000e+02])},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a pipeline - specify the model\n",
    "pipeline = Pipeline([\n",
    "    ('lasso', Lasso(fit_intercept=True))])\n",
    "\n",
    "# we can specify parameters here we want to optimize\n",
    "# notice the format - {'model__parameter': range}, this format should follow for all parameters\n",
    "parameters = {'lasso__alpha': 2.0**(np.arange(-10, 10, 1))}\n",
    "\n",
    "# call the gridsearch method and fit\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=3, refit=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lasso__alpha': 0.001953125}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.863346214002065"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r2 score\n",
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework task 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With trip cost amount as a target variable and using trip distance, travel time and PULocationID as predictors, build a Random Forest Regression with 10 trees (n_estimators) using the sklearn pipeline to select the hyperparameter 'max_depth' with values from 2 to 5.\n",
    "\n",
    "Use 75:25 train test split as above, use the training data for GridSearchCV with three-fold cross-validation (cv=3). Report the best max_depth as well as the R2 on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X as features and y as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
